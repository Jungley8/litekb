# ğŸ“– LiteKB æ¨¡å‹ä½¿ç”¨åœºæ™¯ä¸é…ç½®æŒ‡å—

## ç›®å½•

- [1. æ¨¡å‹ä½¿ç”¨åœºæ™¯](#1-æ¨¡å‹ä½¿ç”¨åœºæ™¯)
- [2. ä¾›åº”å•†å¯¹æ¯”](#2-ä¾›åº”å•†å¯¹æ¯”)
- [3. åœºæ™¯é…ç½®æ¨è](#3-åœºæ™¯é…ç½®æ¨è)
- [4. æˆæœ¬ä¼˜åŒ–ç­–ç•¥](#4-æˆæœ¬ä¼˜åŒ–ç­–ç•¥)
- [5. æ•…éšœæ’æŸ¥](#5-æ•…éšœæ’æŸ¥)

---

## 1. æ¨¡å‹ä½¿ç”¨åœºæ™¯

### 1.1 RAG å¯¹è¯ (æœ€å¸¸ç”¨)

**åœºæ™¯æè¿°**: ç”¨æˆ·åŸºäºçŸ¥è¯†åº“å†…å®¹è¿›è¡Œé—®ç­”

**æŠ€æœ¯æµç¨‹**:
```
ç”¨æˆ·æé—® â†’ æ£€ç´¢ç›¸å…³æ–‡æ¡£ â†’ æ„å»º Prompt â†’ LLM ç”Ÿæˆå›ç­” â†’ è¿”å›ç»“æœ
```

**è´¨é‡è¦æ±‚**:
- âœ… å›ç­”å‡†ç¡®ï¼Œå¼•ç”¨æ¥æº
- âœ… ä¸Šä¸‹æ–‡ç†è§£èƒ½åŠ›å¼º
- âœ… é¿å…å¹»è§‰ (Hallucination)

**é…ç½®è¦ç‚¹**:
```python
{
    "temperature": 0.1,      # ä½æ¸©åº¦ï¼Œå‡å°‘éšæœºæ€§
    "max_tokens": 4000,      # é•¿å›ç­”
    "top_p": 0.95,          # æ ¸é‡‡æ ·
    "stream": True,          # å¼€å¯æµå¼
}
```

---

### 1.2 æ–‡æ¡£æ‘˜è¦ç”Ÿæˆ

**åœºæ™¯æè¿°**: è‡ªåŠ¨ä¸ºé•¿æ–‡æ¡£ç”Ÿæˆæ‘˜è¦ã€å…³é”®ç‚¹ã€åˆ†ç±»

**æŠ€æœ¯æµç¨‹**:
```
æ–‡æ¡£å†…å®¹ â†’ LLM åˆ†æ â†’ æå–æ‘˜è¦ + å…³é”®ç‚¹ + å®ä½“ â†’ å­˜å…¥å…ƒæ•°æ®
```

**è´¨é‡è¦æ±‚**:
- âœ… æå–å…³é”®ä¿¡æ¯å‡†ç¡®
- âœ… åˆ†ç±»åˆç†
- âœ… æ‘˜è¦ç®€æ´

**é…ç½®è¦ç‚¹**:
```python
{
    "temperature": 0.3,      # ä¸­ä½æ¸©åº¦
    "max_tokens": 1000,      # æ‘˜è¦ä¸éœ€è¦å¤ªé•¿
    "model": "gpt-4o",      # éœ€è¦ç†è§£èƒ½åŠ›
}
```

---

### 1.3 çŸ¥è¯†å›¾è°±æ„å»º

**åœºæ™¯æè¿°**: ä»æ–‡æ¡£ä¸­æå–å®ä½“å’Œå…³ç³»

**æŠ€æœ¯æµç¨‹**:
```
æ–‡æ¡£æ®µè½ â†’ NER æå–å®ä½“ â†’ å…³ç³»æŠ½å– â†’ æ„å»ºä¸‰å…ƒç»„ â†’ å­˜å…¥å›¾æ•°æ®åº“
```

**è´¨é‡è¦æ±‚**:
- âœ… å®ä½“è¯†åˆ«å‡†ç¡®
- âœ… å…³ç³»æŠ½å–æ­£ç¡®
- âœ… å®ä½“å»é‡

**é…ç½®è¦ç‚¹**:
```python
{
    "temperature": 0.0,      # æœ€ä½æ¸©åº¦ï¼Œç¡®ä¿ä¸€è‡´æ€§
    "max_tokens": 2000,
    "prompt": "ä¸¥æ ¼æå–å®ä½“å’Œå…³ç³»...",
}
```

---

### 1.4 æ··åˆæ£€ç´¢ (Search)

**åœºæ™¯æè¿°**: å‘é‡ + BM25 + RRF èåˆæ£€ç´¢

**æŠ€æœ¯æµç¨‹**:
```
æŸ¥è¯¢ â†’ å‘é‡æ£€ç´¢ + å…³é”®è¯æ£€ç´¢ â†’ ç»“æœèåˆ â†’ é‡æ’åº â†’ è¿”å› Top-K
```

**è´¨é‡è¦æ±‚**:
- âœ… æ£€ç´¢ç²¾åº¦é«˜
- âœ… è¯­ä¹‰ç†è§£å‡†ç¡®
- âœ… å“åº”é€Ÿåº¦å¿«

**é…ç½®è¦ç‚¹**:
```python
# Embedding æ¨¡å‹é€‰æ‹©
{
    "model": "text-embedding-3-large",  # OpenAI æœ€å¼º
    # æˆ–
    "model": "BAAI/bge-large-zh",      # æœ¬åœ°ä¸­æ–‡æœ€ä¼˜
}
```

---

### 1.5 å†…å®¹æ”¹å†™ä¸æ‰©å±•

**åœºæ™¯æè¿°**: åŸºäºåŸå§‹å†…å®¹ç”Ÿæˆå˜ä½“ã€æ‰©å±•ã€ç¿»è¯‘

**æŠ€æœ¯æµç¨‹**:
```
åŸå§‹å†…å®¹ â†’ Prompt å·¥ç¨‹ â†’ LLM å¤„ç† â†’ è¾“å‡ºç»“æœ
```

**è´¨é‡è¦æ±‚**:
- âœ… ä¿æŒåŸæ„
- âœ… è¡¨è¾¾æµç•…
- âœ… é£æ ¼ä¸€è‡´

**é…ç½®è¦ç‚¹**:
```python
{
    "temperature": 0.7,      # é€‚åº¦åˆ›é€ æ€§
    "max_tokens": 4000,
}
```

---

### 1.6 å¤šæ¨¡æ€å¤„ç†

**åœºæ™¯æè¿°**: å›¾ç‰‡ OCRã€å›¾è¡¨ç†è§£ã€éŸ³é¢‘è½¬å†™

**æŠ€æœ¯æµç¨‹**:
```
å›¾ç‰‡/éŸ³é¢‘ â†’ OCR/ASR â†’ ç»“æ„åŒ–å†…å®¹ â†’ å­˜å…¥çŸ¥è¯†åº“
```

**è´¨é‡è¦æ±‚**:
- âœ… è¯†åˆ«å‡†ç¡®
- âœ… ç‰ˆé¢ç†è§£
- âœ… è¡¨æ ¼æå–

**é…ç½®è¦ç‚¹**:
```python
{
    "model": "gpt-4o",      # è§†è§‰èƒ½åŠ›å¼º
    "temperature": 0.1,
}
```

---

## 2. ä¾›åº”å•†å¯¹æ¯”

### 2.1 ç»¼åˆå¯¹æ¯”

| ä¾›åº”å•† | ä¸­æ–‡æ•ˆæœ | é€Ÿåº¦ | æˆæœ¬ | ç¨³å®šæ€§ | æ¨èåº¦ |
|--------|----------|------|------|--------|--------|
| **OpenAI gpt-4o** | â­â­â­â­â­ | â­â­â­â­ | $$ | â­â­â­â­â­ | â­â­â­â­â­ |
| **OpenAI gpt-4o-mini** | â­â­â­â­ | â­â­â­â­â­ | $ | â­â­â­â­â­ | â­â­â­â­â­ |
| **Anthropic Claude-3.5** | â­â­â­â­ | â­â­â­â­ | $$ | â­â­â­â­ | â­â­â­â­ |
| **Google Gemini-1.5** | â­â­â­â­ | â­â­â­â­ | $$ | â­â­â­ | â­â­â­â­ |
| **Ollama (æœ¬åœ°)** | â­â­â­â­ | â­â­â­ | å…è´¹ | â­â­â­ | â­â­â­â­ |
| **vLLM (æœ¬åœ°)** | â­â­â­â­ | â­â­â­â­â­ | å…è´¹ | â­â­â­ | â­â­â­â­â­ |

### 2.2 Embedding å¯¹æ¯”

| æ¨¡å‹ | ç»´åº¦ | ä¸­æ–‡æ•ˆæœ | é€Ÿåº¦ | æˆæœ¬ |
|------|------|----------|------|------|
| **text-embedding-3-large** | 3072 | â­â­â­â­â­ | å¿« | $ |
| **text-embedding-3-small** | 1536 | â­â­â­â­ | å¿« | $ |
| **BAAI/bge-large-zh** | 1024 | â­â­â­â­â­ | ä¸­ | å…è´¹ |
| **BAAI/bge-base-zh** | 768 | â­â­â­â­ | å¿« | å…è´¹ |

---

## 3. åœºæ™¯é…ç½®æ¨è

### 3.1 RAG å¯¹è¯åœºæ™¯

#### åœºæ™¯ A: é«˜è´¨é‡ç”Ÿäº§ç¯å¢ƒ

**æ¨èä¾›åº”å•†**: OpenAI gpt-4o

```python
config = {
    "provider": "openai",
    "model": "gpt-4o",
    "temperature": 0.1,
    "max_tokens": 4000,
    "top_p": 0.95,
}

# Prompt ä¼˜åŒ–
system_prompt = """ä½ æ˜¯çŸ¥è¯†åº“åŠ©æ‰‹ã€‚åŸºäºä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. åªåŸºäºä¸Šä¸‹æ–‡å›ç­”ï¼Œæ ‡æ³¨å¼•ç”¨æ¥æº
2. å¤æ‚é—®é¢˜åˆ†ç‚¹è¯´æ˜
3. ä¸ç¡®å®šæ—¶æ˜ç¡®è¯´æ˜

ä¸Šä¸‹æ–‡ï¼š{context}
"""
```

**æˆæœ¬**: çº¦ $0.01/æ¬¡å¯¹è¯

---

#### åœºæ™¯ B: æˆæœ¬æ•æ„Ÿç¯å¢ƒ

**æ¨èä¾›åº”å•†**: OpenAI gpt-4o-mini

```python
config = {
    "provider": "openai",
    "model": "gpt-4o-mini",
    "temperature": 0.1,
    "max_tokens": 2000,
}

# æ•ˆæœç•¥é™ä½†æˆæœ¬é™ä½ 10x
```

**æˆæœ¬**: çº¦ $0.001/æ¬¡å¯¹è¯

---

#### åœºæ™¯ C: å®Œå…¨å…è´¹ç¯å¢ƒ

**æ¨èä¾›åº”å•†**: Ollama qwen2.5:7b

```bash
# å¯åŠ¨ Ollama
ollama run qwen2.5:7b
```

```python
config = {
    "provider": "ollama",
    "model": "qwen2.5:7b",
    "temperature": 0.1,
    "max_tokens": 2000,
}
```

**æ•ˆæœè¯„ä¼°**:
- âœ… å…è´¹ï¼Œæ— é™åˆ¶
- â±ï¸ å“åº”é€Ÿåº¦ 2-5 ç§’
- âš ï¸ å¤æ‚é—®é¢˜æ•ˆæœç•¥å·®

---

#### åœºæ™¯ D: é«˜å¹¶å‘ç”Ÿäº§ç¯å¢ƒ

**æ¨èä¾›åº”å•†**: vLLM + Qwen2.5-7B

```bash
# å¯åŠ¨ vLLM
vllm serve Qwen/Qwen2.5-7B-Instruct \
    --tensor-parallel-size 4 \
    --port 8000
```

```python
config = {
    "provider": "vllm",
    "model": "Qwen/Qwen2.5-7B-Instruct",
    "temperature": 0.1,
    "max_tokens": 4000,
}
```

**æ€§èƒ½**:
- âœ… 100+ QPS (4GPU)
- âœ… å®Œå…¨å…è´¹
- âš ï¸ éœ€è¦ GPU èµ„æº

---

### 3.2 Embedding åœºæ™¯

#### é«˜è´¨é‡æ£€ç´¢

**æ¨è**: OpenAI text-embedding-3-large

```python
embedding_config = {
    "model": "text-embedding-3-large",
    "dimensions": 1024,  # å¯å‹ç¼©
}
```

**æˆæœ¬**: $0.00013/1K tokens

---

#### æˆæœ¬ä¼˜åŒ–

**æ¨è**: BAAI/bge-large-zh (æœ¬åœ°)

```python
embedding_config = {
    "model": "BAAI/bge-large-zh",
    "use_local": True,
}
```

**æˆæœ¬**: å…è´¹ (éœ€æœ¬åœ°éƒ¨ç½²)

---

#### ä¸­æ–‡ä¸“ç²¾

**æ¨è**: BAAI/bge-m3

```python
embedding_config = {
    "model": "BAAI/bge-m3",
    "use_local": True,
}
```

---

### 3.3 çŸ¥è¯†å›¾è°±åœºæ™¯

#### é«˜å‡†ç¡®æ€§è¦æ±‚

**æ¨è**: OpenAI gpt-4o

```python
graph_config = {
    "model": "gpt-4o",
    "temperature": 0.0,  # æœ€ä½æ¸©åº¦
    "max_tokens": 2000,
    "prompt": """ä¸¥æ ¼ä»ä»¥ä¸‹æ–‡æœ¬æå–å®ä½“å’Œå…³ç³»ï¼š

æ–‡æœ¬ï¼š{text}

æ ¼å¼ï¼š
å®ä½“ï¼šentity_name | entity_type
å…³ç³»ï¼š(entity1) --relation--> (entity2)
""",
}
```

---

### 3.4 å¤šæ¨¡æ€åœºæ™¯

#### å›¾ç‰‡ OCR

**æ¨è**: OpenAI gpt-4o

```python
multimodal_config = {
    "model": "gpt-4o",
    "temperature": 0.1,
}
```

**æ”¯æŒ**: å›¾ç‰‡ã€PDFã€æ‰«æä»¶

---

#### å…è´¹æ›¿ä»£ (ç®€å• OCR)

**æ¨è**: Tesseract (æœ¬åœ°)

```python
from app.services.ocr import ocr_service

text = await ocr_service.recognize(image_path)
```

**æˆæœ¬**: å…è´¹

---

## 4. æˆæœ¬ä¼˜åŒ–ç­–ç•¥

### 4.1 ç¼“å­˜ç­–ç•¥

```python
# Redis ç¼“å­˜å›ç­”
cache_config = {
    "enabled": True,
    "ttl": 3600,        # 1å°æ—¶
    "similarity_threshold": 0.95,  # é«˜ç›¸ä¼¼åº¦ç›´æ¥è¿”å›
}
```

**èŠ‚çœ**: 30-50% LLM è°ƒç”¨

---

### 4.2 Prompt ä¼˜åŒ–

```python
# âŒ é”™è¯¯ï¼šè¿‡äºå¤æ‚
bad_prompt = "ä½ æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œä½ è¦..."

# âœ… æ­£ç¡®ï¼šç®€æ´æ˜äº†
good_prompt = "åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”ï¼š{context}"
```

**èŠ‚çœ**: 20-30% tokens

---

### 4.3 æ¨¡å‹åˆ†çº§

```python
# ç®€å•é—®é¢˜ â†’ å»‰ä»·æ¨¡å‹
# å¤æ‚é—®é¢˜ â†’ é«˜ç«¯æ¨¡å‹

async def route_question(question: str) -> str:
    if is_simple_question(question):
        return "gpt-4o-mini"
    else:
        return "gpt-4o"
```

**èŠ‚çœ**: 50-70% æˆæœ¬

---

### 4.4 æ··åˆç­–ç•¥

```python
# æ¨èé…ç½®ç»„åˆ

PROFILES = {
    "development": {
        "chat": "ollama/qwen2.5:7b",
        "embedding": "BAAI/bge-large-zh",
        "cost": "å…è´¹",
    },
    
    "staging": {
        "chat": "openai/gpt-4o-mini",
        "embedding": "text-embedding-3-small",
        "cost": "$10/æœˆ",
    },
    
    "production": {
        "chat": "openai/gpt-4o",
        "embedding": "text-embedding-3-large",
        "cost": "$100/æœˆ",
    },
    
    "high_concurrency": {
        "chat": "vllm/Qwen2.5-7B",
        "embedding": "BAAI/bge-large-zh",
        "cost": "GPUæˆæœ¬",
    },
}
```

---

## 5. æ•…éšœæ’æŸ¥

### 5.1 å¸¸è§é—®é¢˜

| é—®é¢˜ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| å›ç­”è´¨é‡å·® | temperature è¿‡é«˜ | é™ä½åˆ° 0.1 |
| å“åº”æ…¢ | æ¨¡å‹å¤ªå¤§ / ç½‘ç»œ | æ¢å°æ¨¡å‹ / æœ¬åœ°éƒ¨ç½² |
| æˆæœ¬è¶…æ”¯ | æ— ç¼“å­˜ / Prompt å†—é•¿ | å¼€å¯ç¼“å­˜ / ä¼˜åŒ– Prompt |
| å¹»è§‰ä¸¥é‡ | ç¼ºä¹ä¸Šä¸‹æ–‡ | å¢åŠ æ£€ç´¢ç»“æœ / é™ä½ temperature |

---

### 5.2 ç›‘æ§æŒ‡æ ‡

```python
# éœ€è¦ç›‘æ§çš„æŒ‡æ ‡

metrics = {
    "llm_calls": "LLM è°ƒç”¨æ¬¡æ•°",
    "avg_latency": "å¹³å‡å“åº”æ—¶é—´",
    "avg_cost": "å¹³å‡æˆæœ¬/æ¬¡",
    "cache_hit_rate": "ç¼“å­˜å‘½ä¸­ç‡",
    "user_satisfaction": "ç”¨æˆ·æ»¡æ„åº¦",
}
```

---

## ğŸ“‹ å¿«é€Ÿé…ç½®å‚è€ƒ

### å¼€å‘æµ‹è¯•

```yaml
# docker-compose.override.yml
services:
  backend:
    environment:
      - LLM_PROVIDER=ollama
      - OLLAMA_URL=http://ollama:11434
      - EMBEDDING_MODEL=BAAI/bge-large-zh
      - USE_LOCAL_EMBEDDING=true

  ollama:
    image: ollama/ollama
    volumes:
      - ollama:/root/.ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

### ç”Ÿäº§ç¯å¢ƒ

```yaml
# docker-compose.prod.yml
services:
  backend:
    environment:
      - LLM_PROVIDER=openai
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - EMBEDDING_MODEL=text-embedding-3-large
```

---

## ğŸ¯ æ€»ç»“

| åœºæ™¯ | é¦–é€‰ä¾›åº”å•† | å¤‡é€‰æ–¹æ¡ˆ |
|------|-----------|----------|
| RAG å¯¹è¯ (è´¨é‡) | OpenAI gpt-4o | Claude-3.5 |
| RAG å¯¹è¯ (æˆæœ¬) | gpt-4o-mini | Ollama |
| RAG å¯¹è¯ (å…è´¹) | Ollama qwen2.5 | vLLM |
| Embedding (è´¨é‡) | text-embedding-3-large | BAAI/bge |
| Embedding (å…è´¹) | BAAI/bge-large-zh | - |
| é«˜å¹¶å‘ | vLLM | - |
| å¤šæ¨¡æ€ | gpt-4o | Gemini-1.5 |
| å›¾è°±æ„å»º | gpt-4o (ä½æ¸©) | Claude-3.5 |
